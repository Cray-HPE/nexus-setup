Copyright 2020 Hewlett Packard Enterprise Development LP


Package Repository Management
=============================


Nexus Setup
-----------

Nexus is deployed according to the `/opt/cray/manifests/nexus-airgap.yaml`
manifest, which is generated by `crayctl pre-install`. This airgap setup
deploys three charts:

| Chart              | Description |
|--------------------|-------------|
| `cray-nexus`       | Deploys Nexus Repository Manager 3 |
| `cray-nexus-setup` | Configures Shasta blob stores and repositories |
| `cray-nexus-sync`  | Syncs assets from the Blob on BIS (i.e., ncn-w001:/var/cray/vbis/www/shasta-cd-repo) to corresponding repositories in Nexus |

The following are the most likely system-specific customizations that customers may choose to set:

| Customization                                                                 | Default               | Description |
|-------------------------------------------------------------------------------|-----------------------|-------------|
| `spec.kubernetes.services.cray-nexus.istio.ingress.hosts.ui.enabled`          | `true`                | Enables ingress from the CAN (default chart value is `false`) |
| `spec.kubernetes.services.cray-nexus.istio.ingress.hosts.ui.authority`        | `{{ dns.uis.nexus }}` | Sets the CAN hostname (default chart value is `nexus.local`) |
| `spec.kubernetes.services.cray-nexus.sonatype-nexus.persistence.storageClass` | _cluster default_     | Set to `ceph-cephfs-external` to use CephFS; otherwise, do not set to use cluster default, which is `k8s-block-replicated` |
| `spec.kubernetes.services.cray-nexus.sonatype-nexus.persistence.storageSize`  | `100Gi`               | Nexus storage size, may be increased after installation; critical if `spec.kubernetes.services.cray-nexus-setup.s3.enabled` is `false` | 
| `spec.kubernetes.services.cray-nexus-setup.s3.enabled`                        | `true`                | If enabled, Nexus S3 blob stores are configured to stores assets (e.g., RPMs, container images, charts) via Ceph RGW; otherwise File blob stores are configured, which store assets in the PV |

However, all three charts support extensive customization as appropriate.
Contact HPE for additional assistance as required.

By default, the Nexus charts are deployed to the `nexus` namespace. A typical
deployment will look similar to the following:

```
# kubectl -n nexus get all
NAME                                                 READY   STATUS      RESTARTS   AGE
pod/cray-precache-images-dzp4c                       2/2     Running     0          2d1h
pod/cray-precache-images-hxnr5                       2/2     Running     0          2d1h
pod/cray-precache-images-m68mt                       2/2     Running     0          2d1h
pod/nexus-55d8c77547-ctqgp                           2/2     Running     0          2d
pod/sync-badger-1.3.0-vn5mg                          0/1     Completed   0          2d
pod/sync-charts-bsnbp                                0/1     Completed   0          2d
pod/sync-cos-1.3.0-sle-15sp1-comp-zft9g              0/1     Completed   0          2d
pod/sync-cos-1.3.0-sle-15sp1-mgmt-t4r8x              0/1     Completed   0          2d
pod/sync-cos-images-1.3.0-sle-15sp1-comp-mt7dg       0/1     Completed   0          2d
pod/sync-ct-tests-1.3.0-sle-15sp1-mgmt-rxpmr         0/1     Completed   0          2d
pod/sync-docker.io-4wqcc                             0/1     Completed   0          2d
pod/sync-docker.io-7hf5g                             0/1     Error       0          2d
pod/sync-dtr.dev.cray.com-wqhm9                      0/1     Error       0          2d
pod/sync-dtr.dev.cray.com-x4k56                      0/1     Completed   0          2d
pod/sync-mirror-1.3.0-opensuse-leap-15-x7fbn         0/1     Completed   0          40h
pod/sync-mirror-1.3.0-sle-15sp1-all-products-5rczb   0/1     Completed   0          22h
pod/sync-mirror-1.3.0-sle-15sp1-all-updates-x22r7    0/1     Completed   0          22h
pod/sync-quay.io-mgnnl                               0/1     Completed   0          2d
pod/sync-shasta-firmware-1.3.0-qlqwl                 0/1     Completed   0          2d
pod/sync-sma-1.3.0-sle-15sp1-comp-6bv4h              0/1     Completed   0          2d
pod/sync-sma-1.3.0-sle-15sp1-mgmt-mfkd9              0/1     Completed   0          2d
pod/sync-sma-ccd-1.3.0-sle-15sp1-mgmt-pphlt          0/1     Completed   0          2d
pod/sync-sms-ccd-1.3.0-sle-15sp1-mgmt-l465f          0/1     Completed   0          2d
pod/sync-thirdparty-1.3.0-sle-15sp1-comp-s8f9b       0/1     Completed   0          2d
pod/sync-thirdparty-1.3.0-sle-15sp1-mgmt-6pjnv       0/1     Completed   0          2d

NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)           AGE
service/nexus   ClusterIP   10.28.148.127   <none>        80/TCP,5003/TCP   2d

NAME                                  DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/cray-precache-images   3         3         3       3            3           <none>          2d1h

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nexus   1/1     1            1           2d

NAME                               DESIRED   CURRENT   READY   AGE
replicaset.apps/nexus-55d8c77547   1         1         1       2d

NAME                                                 COMPLETIONS   DURATION   AGE
job.batch/sync-badger-1.3.0                          1/1           16s        2d
job.batch/sync-charts                                1/1           21s        2d
job.batch/sync-cos-1.3.0-sle-15sp1-comp              1/1           7m47s      2d
job.batch/sync-cos-1.3.0-sle-15sp1-mgmt              1/1           7m45s      2d
job.batch/sync-cos-images-1.3.0-sle-15sp1-comp       1/1           91s        2d
job.batch/sync-ct-tests-1.3.0-sle-15sp1-mgmt         1/1           17s        2d
job.batch/sync-docker.io                             1/1           8m         2d
job.batch/sync-dtr.dev.cray.com                      1/1           63m        2d
job.batch/sync-mirror-1.3.0-opensuse-leap-15         1/1           25m        40h
job.batch/sync-mirror-1.3.0-sle-15sp1-all-products   1/1           19m        22h
job.batch/sync-mirror-1.3.0-sle-15sp1-all-updates    1/1           22m        22h
job.batch/sync-quay.io                               1/1           8m8s       2d
job.batch/sync-shasta-firmware-1.3.0                 1/1           5m49s      2d
job.batch/sync-sma-1.3.0-sle-15sp1-comp              1/1           16s        2d
job.batch/sync-sma-1.3.0-sle-15sp1-mgmt              1/1           15s        2d
job.batch/sync-sma-ccd-1.3.0-sle-15sp1-mgmt          1/1           9s         2d
job.batch/sync-sms-ccd-1.3.0-sle-15sp1-mgmt          1/1           7m27s      2d
job.batch/sync-thirdparty-1.3.0-sle-15sp1-comp       1/1           5m36s      2d
job.batch/sync-thirdparty-1.3.0-sle-15sp1-mgmt       1/1           12s        2d
```

Note: The `cray-precache-images` DaemonSet is used to keep select container
images resident in the image cache on each worker node to ensure Nexus
resiliency. It is not a part of Nexus setup.


### Nexus vs BIS

During installation, zypper and containerd are configured to use repositories
hosted on BIS, i.e., ncn-w001.  At the end of stage 4, configurations for
zypper repositories and containerd registry mirrors are transitioned from BIS
to Neuxs.  The
`/opt/cray/crayctl/ansible_framework/main/nexus-update-clients.yml` playbook is
used to facilitate this switch over, and the
`/opt/cray/crayctl/ansible_framework/main/nexus-reset-clients.yml` playbook may
be used to switch configurations back to BIS.


### Blob Stores

The Shasta 1.3 setup creates `shasta-1.3` and `latest` blob stores. By default,
they are S3 blob stores configured to use the `nexus` bucket in Ceph RGW.
However, if S3 is disabled (see above customization settings), then
`shasta-1.3` and `latest` will be File blob stores, which stores assets in the
Nexus' persistent volume (PV).

After installation, the `shasta-1.3` blob store is expected to be nearly 155GB,
as it contains all Shasta 1.3 assets, whereas `latest` will be around 100MB,
since it only includes metadata for select Yum group repositories.

Future releases will use different blob stores in order to separate assets and
make it more straight-forward to remove deprecated assets from backend storage.


### Repository Formats

Shasta 1.3 includes repositories in `docker`, `helm`, `raw`, and `yum` formats.

#### Docker

In order to access `docker` repositories, Nexus requires additional ports. In
Shasta 1.3, https://registry.local will route to Nexus port 5003, which is the
`registry-1.3` repository. As such, any container image pushed to
regsitry.local will be implicitly managed according to the Shasta 1.3
lifecycle. Expect future releases to create additional `docker` repositories
(e.g., `registry-1.4`) and for pushes to https://registry.local to go to the
registry corresponding to the latest release.

#### Helm

Nexus 3.25.x does not support group repositories for `helm` format, so layering
of Helm repositories is not currently possible. Although the 1.3 installation
loaded charts directly from the Blob, future 1.3 patch releases will require
updating and using the `charts-1.3` repository.

#### Raw

Raw repositories are used to host squashFS images.

#### Yum

_Hosted_ repositories are versioned according the patch release (e.g.,
repositories for initial 1.3 release use 1.3.0). Patch releases will create
additional hosted repositories (e.g., the first patch release will use 1.3.1)
to manage delta updates since the previous patch release. 

_Group_ repositories appropriately combine hosted repositories for patch
releases into a "meta" repository versioned according to the minor release (e.g.,
1.3). It is these repositories that clients (e.g., zypper) are expected to use.


### Populating Repositories

The `cray-nexus-setup` chart deploys a "sync" job per source repository to
upload assets to the corresponding Nexus repository. These jobs run in the
`nexus` namespace and their logs contain metrics for every asset uploaded.

```
ncn-w001:~ # kubectl -n nexus get jobs
NAME                                       COMPLETIONS   DURATION   AGE
sync-badger-1.3.0                          1/1           12s        27h
sync-charts                                1/1           15s        27h
sync-cos-1.3.0-sle-15sp1-comp              1/1           94s        27h
sync-cos-1.3.0-sle-15sp1-mgmt              1/1           68s        27h
sync-cos-images-1.3.0-sle-15sp1-comp       1/1           2m23s      27h
sync-ct-tests-1.3.0-sle-15sp1-mgmt         1/1           15s        27h
sync-docker.io                             1/1           32s        27h
sync-dtr.dev.cray.com                      1/1           21m        27h
sync-mirror-1.3.0-opensuse-leap-15         1/1           16m        27h
sync-mirror-1.3.0-sle-15sp1-all-products   1/1           6m7s       27h
sync-mirror-1.3.0-sle-15sp1-all-updates    1/1           5m8s       27h
sync-quay.io                               1/1           39s        27h
sync-shasta-firmware-1.3.0                 1/1           95s        27h
sync-sma-1.3.0-sle-15sp1-comp              1/1           11s        27h
sync-sma-1.3.0-sle-15sp1-mgmt              1/1           10s        27h
sync-sma-ccd-1.3.0-sle-15sp1-mgmt          1/1           14s        27h
sync-sms-ccd-1.3.0-sle-15sp1-mgmt          1/1           81s        27h
sync-thirdparty-1.3.0-sle-15sp1-comp       1/1           15s        27h
sync-thirdparty-1.3.0-sle-15sp1-mgmt       1/1           9s         27h
```

Depending on various factors, the time it takes for these jobs to complete may
vary from seconds/minutes to hours. Typically, the ones which take the longest
are `sync-mirror-1.3.0-opensuse-leap-15` and `sync-dtr.dev.cray.com` as those
repositories are the largest.

#### Status and Results of Sync Jobs

Helm, Raw, and Yum repositories are synced using `curl` via Nexus' REST API;
however, Docker repositories are synced using `skopeo` via the Docker API v2.
Job logs for Helm, Raw, and Yum repositories contain two lines per asset. The
first indicates the HTTP response code from Nexus, upload size in bytes, total
time in seconds, and upload speed in bytes per second. The second reports a
status of `created` to indicate the asset was created or `exists` to indicate
if the asset already existed:

```
ncn-w001:~ # kubectl -n nexus logs job/sync-mirror-1.3.0-opensuse-leap-15 | head
/blob/bloblets/os/rpms/opensuse/15/external/x86_64/R-sqldf-0.4_11-2.37.x86_64.rpm	204 84710 0.174912 486839.000
/blob/bloblets/os/rpms/opensuse/15/external/x86_64/R-sqldf-0.4_11-2.37.x86_64.rpm	created
/blob/bloblets/os/rpms/opensuse/15/external/x86_64/python2-salt-2019.2.0-lp151.5.18.1.x86_64.rpm	204 7451368 1.289817 5780735.000
/blob/bloblets/os/rpms/opensuse/15/external/x86_64/python2-salt-2019.2.0-lp151.5.18.1.x86_64.rpm	created
/blob/bloblets/os/rpms/opensuse/15/external/x86_64/sssd-tools-1.16.1-lp151.7.16.1.x86_64.rpm	204 278728 0.187893 1490524.000
/blob/bloblets/os/rpms/opensuse/15/external/x86_64/sssd-tools-1.16.1-lp151.7.16.1.x86_64.rpm	created
/blob/bloblets/os/rpms/opensuse/15/external/x86_64/python-Kivy-doc-1.10.1-lp151.2.22.x86_64.rpm	204 21548962 3.374710 6386770.000
/blob/bloblets/os/rpms/opensuse/15/external/x86_64/python-Kivy-doc-1.10.1-lp151.2.22.x86_64.rpm	created
/blob/bloblets/os/rpms/opensuse/15/external/x86_64/libQt5Sql-devel-5.9.7-lp151.4.3.1.x86_64.rpm	204 105098 0.083473 1266240.000
/blob/bloblets/os/rpms/opensuse/15/external/x86_64/libQt5Sql-devel-5.9.7-lp151.4.3.1.x86_64.rpm	created
```

Unfortunately, the Nexus REST API does not distinguish between "asset already
exists" and "that was a bad request for some reason". In both cases it returns
HTTP response 400. So when status `exists` is reported, it must be taken with a
grain of salt. Assuming everything is configured correctly (e.g., the target
repository exists and has the corresponding format, parameters are correct)
then it most likely indicates the asset already exists.

Note: It is possible for an asset to report status `exists` on a new install
due to the way that the upload script and Nexus' REST API function. The scripts
will try to upload an asset until Nexus returns HTTP response 204 or 400.
During an upload Nexus may respond with a transient failure (i.e., HTTP
response 5xx) such that a subsequent upload to a repository that is not
configured to allow updates will respond with HTTP response 400.

```
ncn-w001:~ # kubectl -n nexus logs job/sync-shasta-firmware-1.3.0
+ export NEXUS_URL=https://packages.local
+ NEXUS_URL=https://packages.local
+ nexus-ready
+ nexus-upload-repo-yum /blob/bloblets/shasta-firmware/shasta-firmware/ shasta-firmware-1.3.0
/blob/bloblets/shasta-firmware/shasta-firmware/sh-svr-1264up-bios-crayctldeploy-0.0.12-20200723105942_ceabf49.x86_64.rpm	204 10457 4.144992 2523.000
/blob/bloblets/shasta-firmware/shasta-firmware/sh-svr-1264up-bios-crayctldeploy-0.0.12-20200723105942_ceabf49.x86_64.rpm	created
/blob/bloblets/shasta-firmware/shasta-firmware/sh-svr-3264-bios-crayctldeploy-0.0.12-20200723105942_ceabf49.x86_64.rpm	204 11265 4.193175 2686.000
/blob/bloblets/shasta-firmware/shasta-firmware/sh-svr-3264-bios-crayctldeploy-0.0.12-20200723105942_ceabf49.x86_64.rpm	created
/blob/bloblets/shasta-firmware/shasta-firmware/sh-svr-5264-gpu-bios-crayctldeploy-0.0.12-20200723105942_ceabf49.x86_64.rpm	204 10233 5.243335 1951.000
/blob/bloblets/shasta-firmware/shasta-firmware/sh-svr-5264-gpu-bios-crayctldeploy-0.0.12-20200723105942_ceabf49.x86_64.rpm	created
/blob/bloblets/shasta-firmware/shasta-firmware/sh-svr-3264-bios-20.01.01-20200723105942_ceabf49.x86_64.rpm	400 194895005 18.560338 10500808.000
/blob/bloblets/shasta-firmware/shasta-firmware/sh-svr-3264-bios-20.01.01-20200723105942_ceabf49.x86_64.rpm	exists
/blob/bloblets/shasta-firmware/shasta-firmware/sc-firmware-1.3.355-20200814031818_eecb243-sles15sp1.x86_64.rpm	400 317606331 46.939047 6766363.000
/blob/bloblets/shasta-firmware/shasta-firmware/sc-firmware-1.3.355-20200814031818_eecb243-sles15sp1.x86_64.rpm	exists
/blob/bloblets/shasta-firmware/shasta-firmware/mtn-wnc-rome-bios-1.2.4-20200812162738_d05c182-sles15sp1.x86_64.rpm	400 6010488 52.545259 114387.000
/blob/bloblets/shasta-firmware/shasta-firmware/mtn-wnc-rome-bios-1.2.4-20200812162738_d05c182-sles15sp1.x86_64.rpm	exists
/blob/bloblets/shasta-firmware/shasta-firmware/mtn-ccnc-firmware-1.3.10-20200721192404_6c4e978-sles15sp1.x86_64.rpm	400 123603187 53.665661 2303236.000
/blob/bloblets/shasta-firmware/shasta-firmware/mtn-ccnc-firmware-1.3.10-20200721192404_6c4e978-sles15sp1.x86_64.rpm	exists
/blob/bloblets/shasta-firmware/shasta-firmware/sh-svr-1264up-bios-20.01.01-20200723105942_ceabf49.x86_64.rpm	400 113673877 62.961030 1805464.000
/blob/bloblets/shasta-firmware/shasta-firmware/sh-svr-1264up-bios-20.01.01-20200723105942_ceabf49.x86_64.rpm	exists
/blob/bloblets/shasta-firmware/shasta-firmware/sh-svr-5264-gpu-bios-20.01.01-20200723105942_ceabf49.x86_64.rpm	400 113619125 95.432731 1190576.000
/blob/bloblets/shasta-firmware/shasta-firmware/sh-svr-5264-gpu-bios-20.01.01-20200723105942_ceabf49.x86_64.rpm	exists
real	30m 42.56s
user	7m 25.24s
sys	6m 38.05s
```

Note that all `sync-` jobs for Helm, Raw, and Yum repositories are run with
`time` and will report `real`, `user`, and `sys` times when completed. Jobs
syncing to Docker repositories use `skopeo` which reports an _index_/_total_
for each container image. For example, job `sync-dtr.dev.cray.com` will
upload a total of 214 images:

```
ncn-w001:~ # kubectl -n nexus logs job/sync-dtr.dev.cray.com
time="2020-08-21T18:41:39Z" level=info msg="Copying image tag 1/214" from="dir:/blob/container-images/dtr.dev.cray.com/acid/postgres-operator-ui:v1.5.0-dirty" to="docker://registry.local/acid/postgres-operator-ui:v1.5.0-dirty"
Getting image source signatures
Copying blob sha256:02240d714d472af0fb48263f16b7c675a961acf3ade858ed850d01b9481813c4
Copying blob sha256:2ea52614ba55cca2e7e7ef3f99eda959cca22536dc99db2fa60f2dfc3410a648
Copying blob sha256:440feef1803767d421e7cbef990d0adac26d4ea1a9d349972cd2e6773c209790
Copying blob sha256:c8ec3bf0386aad5d00e261eba381f5b00bc6c3fac46575e9122052d2608f04a9
Copying blob sha256:5d46f528af66c64ef51dd7cfb3caa19e1b0b620f5b20bb8270f2489e6ca4d6cf
Copying blob sha256:485539a11a49d0d429e9f2c4ab851b8439426570052d782dde3bed3f93149127
Copying blob sha256:79ddcb72d3843ac9dc0ea3dc3939883f817a92485525d6b4b3a45b35c9968844
Copying config sha256:f619866cb097e320adc1614ca9a0743da4bef64c4097d1ea3109018274a5d349
Writing manifest to image destination
Storing signatures
...
time="2020-08-21T19:00:58Z" level=info msg="Copying image tag 214/214" from="dir:/blob/container-images/dtr.dev.cray.com/weaveworks/weave-npc:2.6.0" to="docker://registry.local/weaveworks/weave-npc:2.6.0"
Getting image source signatures
Copying blob sha256:6faac30ef2985835c0636c97fb83a3c3808d52ee675ca0ebeb10c909942e447c
Copying blob sha256:f07062b1be21805b663caca8c82b990b5003046b0d5aef0a2829e5283a4895b3
Copying blob sha256:a65ccbe4f5b6d7b0282be891861e0e4df950f4469160cb73aea20db4b4c3bd9a
Copying blob sha256:8fe3ac4806ba4672d6fa6d528c16b5833ac640bd1e27c0b5124cf6a7f3884471
Copying blob sha256:eb2482fb1ad68519e6436a10b39d6b07387f8b0dff7df38ecbc18febc02c6d02
Copying config sha256:5105e13e253e39ae04cc11f0f5395f314c8b470df55d089419f1b014ab92f6f0
Writing manifest to image destination
Storing signatures
time="2020-08-21T19:00:59Z" level=info msg="Synced 214 images from 1 sources"
```

#### Restarting Sync Jobs

For the most part `sync-` jobs are fairly resilient; however, sometimes
things just don't work, and it's convenient to rerun jobs. Unfortunately
Kubernetes doesn't have a "re-run job" easy-button; however, existing jobs can
be modified and replaced to achieve the "re-run" effect. Kubernetes relates
Jobs to Pods using a selector on the `controller-uid` label. So the work-around
is to replace the Job with itself after removing the current selector and
`controller-uid`. For example, to re-run the `sync-dtr.dev.cray.com` job:

```
ncn-w001:~ # kubectl get job -o json -n nexus sync-dtr.dev.cray.com \
>     | jq 'del(.spec.selector)' \
>     | jq 'del(.spec.template.metadata.labels."controller-uid")' \
>     | kubectl replace --force -f -
job.batch "sync-dtr.dev.cray.com" deleted
job.batch/sync-dtr.dev.cray.com replaced
```

Since this can be rather cumbersome, consider creating a shell function to restart Kubernetes Jobs:

```
restart-k8s-job() {
    kubectl get job -o json "$@" \
        | jq 'del(.spec.selector)' \
        | jq 'del(.spec.template.metadata.labels."controller-uid")' \
        | kubectl replace --force -f -
}
```

Now restarting `sync-dtr.dev.cray.com` is much simpler:

```
ncn-w001:~ # restart-k8s-job -n nexus sync-dtr.dev.cray.com
job.batch "sync-dtr.dev.cray.com" deleted
job.batch/sync-dtr.dev.cray.com replaced
```


### Yum repository metadata

Nexus may have trouble (re)generating metadata files for Yum repositories,
especially for the larger repositories (e.g., mirror-1.3.0-opensuse-leap-15,
mirror-1.3.0-sle-15sp1-all-products, mirror-1.3.0-sle-15sp1-all-updates). In
order to ensure Yum repository metadata is correct, compare the size of
`repodata/*.xml.gz` files against those in
ncn-w001:/var/cray/vbis/www/shasta-cd-repo. If they are inconsistent, create
Nexus tasks to repair the metadata.

#### Verify Yum repository metadata

It is recommended to verify the size of `*.xml.gz` index files against what is
in the Blob, by default at ncn-w001:/var/cray/vbis/www/shasta-cd-repo. To
generate a list of `*.xml.gz` files in the Blob with their respective
human-readable size:

```
ncn-w001:/var/cray/vbis/www/shasta-cd-repo # find . -name '*.xml.gz' -printf '%s\t%P\n' | numfmt --to=iec-i --padding=8
```

For convenience, the table below lists the name of each Nexus Yum repository
along with it's corresponding path in the Blob as well as the size of relavent
`.xml.gz` indexes. Note that when comparing against indexes in Nexus the
checksum prefixes of each file will be different, but each `repodata/`
directory should have `*-filelists.xml.gz`, `*-other.xml.gz`, and
`*-primary.xml.gz` index files. Similarly, the exact sizes will vary, but they
should be approximately the same size. In practice, if the Nexus index files
are incomplete the size will be off by an order of magnitude. See the next
section on repairing indexes if the difference in size between Blob and Nexus
index files suggest Nexus indexes are incomplete.

| Nexus repository                             | Blob path                                                            | `repodata/*-filelists.xml.gz` | `repodata/*-other.xml.gz` | `repodata/*-primary.xml.gz` |
|----------------------------------------------|----------------------------------------------------------------------|------------------------------:|--------------------------:|----------------------------:|
| badger-1.3.0                                 | `bloblets/badger-shasta-1.3/badger/`                                 | 1.1Ki | 1.3Ki | 2.2Ki |
| cos-1.3.0-sle-15sp1-compute                  | `bloblets/shasta-general-shasta-1.3/rpms/cray-sles15-sp1-cn/`        | 499Ki |  30Ki | 380Ki |
| cos-1.3.0-sle-15sp1-management               | `bloblets/shasta-general-shasta-1.3/rpms/cray-sles15-sp1-ncn/`       | 109Ki |  30Ki | 150Ki |
| ct-tests-1.3.0-sle-15sp1-management          | `bloblets/ct-tests-shasta-1.3/sle15_sp1_ncn/`                        | 3.4Ki | 1.8Ki | 3.8Ki |
| mirror-1.3.0-opensuse-leap-15                | `bloblets/os/rpms/opensuse/15/`                                      |  60Mi |  28Mi |  33Mi |
| mirror-1.3.0-sle-15sp1-all-products          | `bloblets/os/rpms/sles/15sp1-all/Products/`                          | 6.0Mi | 5.0Mi | 3.9Mi |
| mirror-1.3.0-sle-15sp1-all-updates           | `bloblets/os/rpms/sles/15sp1-all/Updates/`                           |  20Mi | 4.2Mi | 7.9Mi |
| shasta-firmware-1.3.0                        | `bloblets/shasta-firmware-shasta-1.3/shasta-firmware/`               | 2.8Ki | 1.3Ki | 2.4Ki |
| sma-1.3.0-sle-15sp1-compute                  | `bloblets/sma-shasta-1.3/rpms/cray-sles15-sp1-cn/`                   | 2.1Ki |   663 | 2.7Ki |
| sma-1.3.0-sle-15sp1-management               | `bloblets/sma-shasta-1.3/rpms/cray-sles15-sp1-ncn/`                  | 2.2Ki |   663 | 2.7Ki |
| sma-crayctldeploy-1.3.0-sle-15sp1-management | `bloblets/sma-shasta-1.3/crayctldeploy/`                             |   966 |   471 | 1.2Ki |
| sms-crayctldeploy-1.3.0-sle-15sp1-management | `bloblets/shasta-general-shasta-1.3/crayctldeploy/`                  |  31Ki |  11Ki |  29Ki |
| thirdparty-1.3.0-sle-15sp1-management        | `bloblets/shasta-general-shasta-1.3/rpms/third-party-sles15sp1-ncn/` |   123 |   123 |   134 |
| thirdparty-1.3.0-sle-15sp1-compute           | `bloblets/shasta-general-shasta-1.3/rpms/third-party-sles15sp1-cn/`  | 1.4Ki |   353 | 1.5Ki |

#### Repairing repodata indexes of Yum repositories

Although [Nexus documentation on Yum
repositories](https://help.sonatype.com/repomanager3/formats/yum-repositories#YumRepositories-DeployingPackagestoYumHostedRepositories)
states that repairing Yum metadata is not typically required, Nexus appears to
have difficulty generating metadata for larger repositories as the `sync-`
jobs run.

> The task Repair - Rebuild Yum repository metadata (repodata) can also be
> configured to create the metadata if the standard generation fails. This is
> not typically needed, thus is regarded as a Repair task.

See the [Nexus documentation on
Tasks](https://help.sonatype.com/repomanager3/system-configuration/tasks) for
more details, but the following screenshots show how to create a repair task to
rebuild Yum metadata for the `mirror-1.3.0-opensuse-leap-15` repository.

![Welcome](screenshots/01-welcome.png)
![Admin](screenshots/02-admin.png)
![Tasks](screenshots/03-tasks.png)
![Create task](screenshots/04-create-task.png)
![Create task - repair Yum metadata](screenshots/05-create-task-repair-yum-metadata.png)
![Create task - repair Yum metadata for mirror-1.3.0-opensuse-leap-15](screenshots/06-create-task-repair-yum-metadata-opensuse.png)
![Tasks, with repair Yum metadata for mirror-1.3.0-opensuse-leap-15](screenshots/07-tasks2.png)
![Task detail - repair Yum metadata for mirror-1.3.0-opensuse-leap-15](screenshots/08-task-detail-opensuse.png)
![Task detail - confirm run](screenshots/09-task-detail-confirm-run-opensuse.png)
![Task detail - last ran](screenshots/10-task-detail-last-ran-opensuse.png)
![Browse repositories](screenshots/11-browse.png)
![Browse mirror-1.3.0-opensuse-leap-15, before metadata rebuilt (top)](screenshots/12-browse-opensuse-before1.png)
![Browse mirror-1.3.0-opensuse-leap-15, before metadata rebuilt (bottom)](screenshots/13-browse-opensuse-before2.png)

Warning: There is [a known issue where repair tasks for rebuilding Yum
repositories report being complete but are not actually
finished](https://issues.sonatype.org/browse/NEXUS-17263). Instead, there is a
`Finished rebuilding yum metadata for repository` log message that indicates it
is finshed, but additional errors may follow it indicating it wasn't succesful.

Even though the Nexus logs contains messages pertaining to tasks, it can be
difficult to track messages for a specific task, especially since rebuilding
Yum metadata takes a long time. In order to more easily track log messages
related to a specific task, `kubectl exec` into the running `nexus` pod and
`cat` the corresponding log file in `/nexus-data/log/tasks`. For example:

```
ncn-w001:~ # kubectl -n nexus get pods | grep nexus
nexus-55d8c77547-65k6q                           2/2     Running     1          22h
```

```
ncn-w001:~ # kubectl -n nexus exec -ti nexus-55d8c77547-65k6q -c nexus -- ls -ltr /nexus-data/log/tasks
total 8
-rw-r--r-- 1 nexus nexus 1763 Aug 23 00:50 repository.yum.rebuild.metadata-20200822235306934.log
-rw-r--r-- 1 nexus nexus 1525 Aug 23 01:00 repository.cleanup-20200823010000013.log
```

If multiple repositories are being rebuilt, search the logs for the specific
repository to find the latest corresponding log file. E.g., for
`mirror-1.3.0-opensuse-leap-15`:

```
ncn-w001:~ # kubectl -n nexus exec -ti nexus-55d8c77547-65k6q -c nexus -- grep -R 'Rebuilding yum metadata for repository mirror-1.3.0-opensuse-leap-15' /nexus-data/log/tasks
/nexus-data/log/tasks/repository.yum.rebuild.metadata-20200822235306934.log:2020-08-22 23:53:06,936+0000 INFO  [event-12-thread-797]  *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl - Rebuilding yum metadata for repository mirror-1.3.0-opensuse-leap-15
```

The log file of a successful rebuild will look similar to this: 

```
ncn-w001:~ # kubectl -n nexus exec -ti nexus-55d8c77547-65k6q -c nexus -- cat /nexus-data/log/tasks/repository.yum.rebuild.metadata-20200822235306934.log
2020-08-22 23:53:06,934+0000 INFO  [quartz-9-thread-20]  *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask - Task information:
2020-08-22 23:53:06,935+0000 INFO  [quartz-9-thread-20]  *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask -  ID: 35536bcd-3947-4ba9-8d6d-43dcadbb87ad
2020-08-22 23:53:06,935+0000 INFO  [quartz-9-thread-20]  *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask -  Type: repository.yum.rebuild.metadata
2020-08-22 23:53:06,935+0000 INFO  [quartz-9-thread-20]  *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask -  Name: Rebuild Yum metadata - mirror-1.3.0-opensuse-leap-15
2020-08-22 23:53:06,935+0000 INFO  [quartz-9-thread-20]  *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask -  Description: Rebuild metadata for mirror-1.3.0-opensuse-leap-15
2020-08-22 23:53:06,936+0000 INFO  [quartz-9-thread-20]  *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask - Task log: /nexus-data/log/tasks/repository.yum.rebuild.metadata-20200822235306934.log
2020-08-22 23:53:06,936+0000 INFO  [event-12-thread-797]  *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl - Rebuilding yum metadata for repository mirror-1.3.0-opensuse-leap-15
2020-08-22 23:53:06,936+0000 INFO  [quartz-9-thread-20]  *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask - Task complete
2020-08-23 00:50:47,468+0000 INFO  [event-12-thread-797]  *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl - Finished rebuilding yum metadata for repository mirror-1.3.0-opensuse-leap-15
```

Notice that the `Finished rebuilding yum metadata for repository` without any
other `ERROR` or `WARN` messages indicates the rebuild has completed
successfully.  In this case it took nearly **58 minutes** to finish! The time
it takes to run is related to the size of the repository, so expect the
`mirror-1.3.0-` repositories to take a while.

![Browse mirror-1.3.0-opensuse-leap-15, before metadata rebuilt (top)](screenshots/14-browse-opensuse-after1.png)
![Browse mirror-1.3.0-opensuse-leap-15, before metadata rebuilt (bottom)](screenshots/15-browse-opensuse-after2.png)


#### Rebuild failures

When a rebuild fails, expect to see `ERROR` and `WARN` messages around the same
time as the `Finished rebuilding yum metadata for repository` message. For
example, consider the log from a failed rebuild of
`mirror-1.3.0-opensuse-leap-15`:

```
ncn-w001:~ # kubectl -n nexus exec -ti nexus-55d8c77547-65k6q -c nexus -- cat /nexus-data/log/tasks/repository.yum.rebuild.metadata-20200822231259523.log
2020-08-22 23:12:59,523+0000 INFO  [quartz-9-thread-20]  *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask - Task information:
2020-08-22 23:12:59,526+0000 INFO  [quartz-9-thread-20]  *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask -  ID: 35536bcd-3947-4ba9-8d6d-43dcadbb87ad
2020-08-22 23:12:59,526+0000 INFO  [quartz-9-thread-20]  *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask -  Type: repository.yum.rebuild.metadata
2020-08-22 23:12:59,526+0000 INFO  [quartz-9-thread-20]  *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask -  Name: Rebuild Yum metadata - mirror-1.3.0-opensuse-leap-15
2020-08-22 23:12:59,527+0000 INFO  [quartz-9-thread-20]  *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask -  Description: Rebuild metadata for mirror-1.3.0-opensuse-leap-15
2020-08-22 23:12:59,529+0000 INFO  [quartz-9-thread-20]  *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask - Task log: /nexus-data/log/tasks/repository.yum.rebuild.metadata-20200822231259523.log
2020-08-22 23:12:59,529+0000 INFO  [event-12-thread-780]  *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl - Rebuilding yum metadata for repository mirror-1.3.0-opensuse-leap-15
2020-08-22 23:12:59,531+0000 INFO  [quartz-9-thread-20]  *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.task.YumCreateRepoTask - Task complete
2020-08-22 23:24:16,974+0000 INFO  [Thread-1948 <command>sql.select from asset where (component IS NOT NULL  AND attributes.yum.asset_kind = :p0) and (bucket=#59:1)</command>]  *SYSTEM com.orientechnologies.common.profiler.OProfilerStub - $ANSI{green {db=component}} [TIP] Query 'SELECT FROM asset WHERE (component IS NOT NULL AND attributes.yum.asset_kind = "RPM" ) AND (bucket = #59:1 )' returned a result set with more than 10000 records. Check if you really need all these records, or reduce the resultset by using a LIMIT to improve both performance and used RAM
2020-08-22 23:29:57,700+0000 INFO  [event-12-thread-780]  *SYSTEM org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl - Finished rebuilding yum metadata for repository mirror-1.3.0-opensuse-leap-15
2020-08-22 23:29:57,701+0000 ERROR [event-12-thread-780]  *SYSTEM com.google.common.eventbus.EventBus.nexus.async - Could not dispatch event org.sonatype.nexus.repository.yum.internal.createrepo.YumMetadataInvalidationEvent@75b487e7 to subscriber org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl$$EnhancerByGuice$$9db995@93053b8 method [public void org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl.on(org.sonatype.nexus.repository.yum.internal.createrepo.YumMetadataInvalidationEvent)]
org.sonatype.nexus.repository.InvalidContentException: Invalid RPM: external/noarch/redeclipse-data-1.5.6-lp151.2.5.noarch.rpm
	at org.sonatype.nexus.repository.yum.internal.rpm.YumRpmParser.parse(YumRpmParser.java:108)
	at org.sonatype.nexus.repository.yum.internal.rpm.YumRpmParser.parse(YumRpmParser.java:76)
	at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.writeAssetToMetadata(CreateRepoServiceImpl.java:651)
	at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.appendRpm(CreateRepoServiceImpl.java:511)
	at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.loopAllRpmsAndAppend(CreateRepoServiceImpl.java:499)
	at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.writeMetadata(CreateRepoServiceImpl.java:477)
	at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.convertDirectoriesToMetadata(CreateRepoServiceImpl.java:180)
	at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.buildMetadata(CreateRepoServiceImpl.java:150)
	at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.buildMetadata(CreateRepoServiceImpl.java:134)
	at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.buildMetadataWithoutCaching(CreateRepoServiceImpl.java:125)
	at org.sonatype.nexus.transaction.TransactionalWrapper.proceedWithTransaction(TransactionalWrapper.java:57)
	at org.sonatype.nexus.transaction.TransactionInterceptor.proceedWithTransaction(TransactionInterceptor.java:66)
	at org.sonatype.nexus.transaction.TransactionInterceptor.invoke(TransactionInterceptor.java:55)
	at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl.buildMetadata(CreateRepoFacetImpl.java:196)
	at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl.on(CreateRepoFacetImpl.java:178)
	at sun.reflect.GeneratedMethodAccessor125.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.google.common.eventbus.Subscriber.invokeSubscriberMethod(Subscriber.java:87)
	at com.google.common.eventbus.Subscriber$SynchronizedSubscriber.invokeSubscriberMethod(Subscriber.java:144)
	at com.google.common.eventbus.Subscriber$1.run(Subscriber.java:72)
	at org.sonatype.nexus.thread.internal.MDCAwareRunnable.run(MDCAwareRunnable.java:40)
	at org.apache.shiro.subject.support.SubjectRunnable.doRun(SubjectRunnable.java:120)
	at org.apache.shiro.subject.support.SubjectRunnable.run(SubjectRunnable.java:108)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.SocketTimeoutException: Read timed out
	at java.net.SocketInputStream.socketRead0(Native Method)
	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
	at java.net.SocketInputStream.read(SocketInputStream.java:171)
	at java.net.SocketInputStream.read(SocketInputStream.java:141)
	at org.apache.http.impl.io.SessionInputBufferImpl.streamRead(SessionInputBufferImpl.java:137)
	at org.apache.http.impl.io.SessionInputBufferImpl.read(SessionInputBufferImpl.java:198)
	at org.apache.http.impl.io.ContentLengthInputStream.read(ContentLengthInputStream.java:176)
	at org.apache.http.conn.EofSensorInputStream.read(EofSensorInputStream.java:135)
	at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
	at com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:180)
	at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
	at com.amazonaws.services.s3.internal.S3AbortableInputStream.read(S3AbortableInputStream.java:125)
	at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
	at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
	at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
	at com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:180)
	at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
	at com.amazonaws.util.LengthCheckInputStream.read(LengthCheckInputStream.java:107)
	at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at com.google.common.io.CountingInputStream.read(CountingInputStream.java:63)
	at java.security.DigestInputStream.read(DigestInputStream.java:161)
	at java.io.FilterInputStream.read(FilterInputStream.java:107)
	at com.google.common.io.ByteStreams.exhaust(ByteStreams.java:273)
	at org.sonatype.nexus.repository.yum.internal.rpm.YumRpmParser.parse(YumRpmParser.java:97)
	... 26 common frames omitted
2020-08-22 23:30:06,427+0000 WARN  [Thread-1948 <command>sql.select from asset where (component IS NOT NULL  AND attributes.yum.asset_kind = :p0) and (bucket=#59:1)</command>]  *SYSTEM org.sonatype.nexus.repository.storage.OrientAsyncHelper$QueueFeedingResultListener - Timed out adding query result to queue 28dee0bf after 60 seconds, aborting query
2020-08-22 23:31:06,430+0000 WARN  [Thread-1948 <command>sql.select from asset where (component IS NOT NULL  AND attributes.yum.asset_kind = :p0) and (bucket=#59:1)</command>]  *SYSTEM org.sonatype.nexus.repository.storage.OrientAsyncHelper$QueueFeedingResultListener - Timed out adding end marker to queue 28dee0bf after 60 seconds
```

Any SQL warnings or notifications indicate the rebuild may have failed failed.
Examine `repodata/*.xml.gz` file attributes such as file size and last modified
time to determine if they are new, compared to the timestamp on the `Finished
rebuilding yum metadata for repository` message.

```
2020-08-22 23:24:16,974+0000 INFO  [Thread-1948 <command>sql.select from asset where (component IS NOT NULL  AND attributes.yum.asset_kind = :p0) and (bucket=#59:1)</command>]  *SYSTEM com.orientechnologies.common.profiler.OProfilerStub - $ANSI{green {db=component}} [TIP] Query 'SELECT FROM asset WHERE (component IS NOT NULL AND attributes.yum.asset_kind = "RPM" ) AND (bucket = #59:1 )' returned a result set with more than 10000 records. Check if you really need all these records, or reduce the resultset by using a LIMIT to improve both performance and used RAM
...
2020-08-22 23:30:06,427+0000 WARN  [Thread-1948 <command>sql.select from asset where (component IS NOT NULL  AND attributes.yum.asset_kind = :p0) and (bucket=#59:1)</command>]  *SYSTEM org.sonatype.nexus.repository.storage.OrientAsyncHelper$QueueFeedingResultListener - Timed out adding query result to queue 28dee0bf after 60 seconds, aborting query
2020-08-22 23:31:06,430+0000 WARN  [Thread-1948 <command>sql.select from asset where (component IS NOT NULL  AND attributes.yum.asset_kind = :p0) and (bucket=#59:1)</command>]  *SYSTEM org.sonatype.nexus.repository.storage.OrientAsyncHelper$QueueFeedingResultListener - Timed out adding end marker to queue 28dee0bf after 60 seconds
```

However, seeing an `ERROR` with a JVM stack trace is a dead giveaway that the
rebuild failed:

```
2020-08-22 23:29:57,701+0000 ERROR [event-12-thread-780]  *SYSTEM com.google.common.eventbus.EventBus.nexus.async - Could not dispatch event org.sonatype.nexus.repository.yum.internal.createrepo.YumMetadataInvalidationEvent@75b487e7 to subscriber org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl$$EnhancerByGuice$$9db995@93053b8 method [public void org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl.on(org.sonatype.nexus.repository.yum.internal.createrepo.YumMetadataInvalidationEvent)]
org.sonatype.nexus.repository.InvalidContentException: Invalid RPM: external/noarch/redeclipse-data-1.5.6-lp151.2.5.noarch.rpm
	at org.sonatype.nexus.repository.yum.internal.rpm.YumRpmParser.parse(YumRpmParser.java:108)
	at org.sonatype.nexus.repository.yum.internal.rpm.YumRpmParser.parse(YumRpmParser.java:76)
	at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.writeAssetToMetadata(CreateRepoServiceImpl.java:651)
	at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.appendRpm(CreateRepoServiceImpl.java:511)
	at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.loopAllRpmsAndAppend(CreateRepoServiceImpl.java:499)
	at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.writeMetadata(CreateRepoServiceImpl.java:477)
	at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.convertDirectoriesToMetadata(CreateRepoServiceImpl.java:180)
	at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.buildMetadata(CreateRepoServiceImpl.java:150)
	at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.buildMetadata(CreateRepoServiceImpl.java:134)
	at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.buildMetadataWithoutCaching(CreateRepoServiceImpl.java:125)
	at org.sonatype.nexus.transaction.TransactionalWrapper.proceedWithTransaction(TransactionalWrapper.java:57)
	at org.sonatype.nexus.transaction.TransactionInterceptor.proceedWithTransaction(TransactionInterceptor.java:66)
	at org.sonatype.nexus.transaction.TransactionInterceptor.invoke(TransactionInterceptor.java:55)
	at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl.buildMetadata(CreateRepoFacetImpl.java:196)
	at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl.on(CreateRepoFacetImpl.java:178)
	at sun.reflect.GeneratedMethodAccessor125.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.google.common.eventbus.Subscriber.invokeSubscriberMethod(Subscriber.java:87)
	at com.google.common.eventbus.Subscriber$SynchronizedSubscriber.invokeSubscriberMethod(Subscriber.java:144)
	at com.google.common.eventbus.Subscriber$1.run(Subscriber.java:72)
	at org.sonatype.nexus.thread.internal.MDCAwareRunnable.run(MDCAwareRunnable.java:40)
	at org.apache.shiro.subject.support.SubjectRunnable.doRun(SubjectRunnable.java:120)
	at org.apache.shiro.subject.support.SubjectRunnable.run(SubjectRunnable.java:108)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.SocketTimeoutException: Read timed out
	at java.net.SocketInputStream.socketRead0(Native Method)
	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
	at java.net.SocketInputStream.read(SocketInputStream.java:171)
	at java.net.SocketInputStream.read(SocketInputStream.java:141)
	at org.apache.http.impl.io.SessionInputBufferImpl.streamRead(SessionInputBufferImpl.java:137)
	at org.apache.http.impl.io.SessionInputBufferImpl.read(SessionInputBufferImpl.java:198)
	at org.apache.http.impl.io.ContentLengthInputStream.read(ContentLengthInputStream.java:176)
	at org.apache.http.conn.EofSensorInputStream.read(EofSensorInputStream.java:135)
	at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
	at com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:180)
	at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
	at com.amazonaws.services.s3.internal.S3AbortableInputStream.read(S3AbortableInputStream.java:125)
	at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
	at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
	at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
	at com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:180)
	at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
	at com.amazonaws.util.LengthCheckInputStream.read(LengthCheckInputStream.java:107)
	at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at com.google.common.io.CountingInputStream.read(CountingInputStream.java:63)
	at java.security.DigestInputStream.read(DigestInputStream.java:161)
	at java.io.FilterInputStream.read(FilterInputStream.java:107)
	at com.google.common.io.ByteStreams.exhaust(ByteStreams.java:273)
	at org.sonatype.nexus.repository.yum.internal.rpm.YumRpmParser.parse(YumRpmParser.java:97)
	... 26 common frames omitted
```
